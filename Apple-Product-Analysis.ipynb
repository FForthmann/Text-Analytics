{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FForthmann/Text-Analytics/blob/main/Apple-Product-Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Datenbeschaffung**"
      ],
      "metadata": {
        "id": "pVSMqn-QLpfR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bibliotheken laden"
      ],
      "metadata": {
        "id": "EMWXNDa2pXP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import re\n",
        "import ast\n",
        "!pip install matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "from collections import Counter\n",
        "from spacy.lang.de.stop_words import STOP_WORDS as stopwords # DE\n",
        "from spacy.language import Language\n",
        "!python -m spacy download de_core_news_sm\n",
        "!pip install spacy spacy-langdetect\n",
        "!pip install pyldavis\n",
        "from spacy_langdetect import LanguageDetector\n",
        "nlp = spacy.load('de_core_news_sm')\n",
        "import torch\n",
        "!pip install textblob-de\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "!pip install germansentiment\n",
        "from germansentiment import SentimentModel\n",
        "\n"
      ],
      "metadata": {
        "id": "mwGOgvKJpWuK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2295fc5d-8624-44c3-858b-002b2f64384e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.43.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "2023-10-08 17:11:07.617303: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting de-core-news-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.6.0/de_core_news_sm-3.6.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from de-core-news-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.1.3)\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Collecting spacy-langdetect\n",
            "  Downloading spacy_langdetect-0.1.2-py3-none-any.whl (5.0 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from spacy-langdetect) (7.4.2)\n",
            "Collecting langdetect==1.0.7 (from spacy-langdetect)\n",
            "  Downloading langdetect-1.0.7.zip (998 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m998.1/998.1 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect==1.0.7->spacy-langdetect) (1.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->spacy-langdetect) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->spacy-langdetect) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->spacy-langdetect) (1.1.3)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->spacy-langdetect) (2.0.1)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.7-py3-none-any.whl size=993412 sha256=514c04517f34d0e67000fc431b6e1cb28c795f1b25a8962cd9cbc83be7f51de8\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/f1/e4/8b73f7a0421b132755956892d29b1e764d3e0857a6e92e32fe\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect, spacy-langdetect\n",
            "Successfully installed langdetect-1.0.7 spacy-langdetect-0.1.2\n",
            "Collecting pyldavis\n",
            "  Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy>=1.24.2 (from pyldavis)\n",
            "  Downloading numpy-1.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyldavis) (1.11.3)\n",
            "Collecting pandas>=2.0.0 (from pyldavis)\n",
            "  Downloading pandas-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyldavis) (1.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyldavis) (3.1.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyldavis) (2.8.7)\n",
            "Collecting funcy (from pyldavis)\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyldavis) (1.2.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from pyldavis) (4.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyldavis) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyldavis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyldavis) (2023.3.post1)\n",
            "Collecting tzdata>=2022.1 (from pandas>=2.0.0->pyldavis)\n",
            "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->pyldavis) (3.2.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->pyldavis) (6.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->pyldavis) (2.1.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyldavis) (1.16.0)\n",
            "Installing collected packages: funcy, tzdata, numpy, pandas, pyldavis\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.1.1 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.26.0 which is incompatible.\n",
            "tensorflow 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 1.26.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed funcy-2.0 numpy-1.26.0 pandas-2.1.1 pyldavis-3.4.1 tzdata-2023.3\n",
            "Collecting textblob-de\n",
            "  Downloading textblob_de-0.4.3-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: textblob>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from textblob-de) (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob>=0.9.0->textblob-de) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob>=0.9.0->textblob-de) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob>=0.9.0->textblob-de) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob>=0.9.0->textblob-de) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob>=0.9.0->textblob-de) (4.66.1)\n",
            "Installing collected packages: textblob-de\n",
            "Successfully installed textblob-de-0.4.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting germansentiment\n",
            "  Downloading germansentiment-1.1.0-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from germansentiment) (2.0.1+cu118)\n",
            "Collecting transformers (from germansentiment)\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->germansentiment) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->germansentiment) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->germansentiment) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->germansentiment) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->germansentiment) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->germansentiment) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->germansentiment) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->germansentiment) (17.0.2)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers->germansentiment)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->germansentiment) (1.26.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->germansentiment) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->germansentiment) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->germansentiment) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->germansentiment) (2.31.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Daten laden"
      ],
      "metadata": {
        "id": "08_l5xAAqMZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Einfügen API-Schlüssel\n",
        "#API_SCHLÜSSEL = '18b9fa2ad3e747caa25adcbb0de2bc05' # Lukas\n",
        "#API_SCHLÜSSEL = '6279420147a54e6bbdfa978580ca2eb0' # Denik\n",
        "#API_SCHLÜSSEL = '60c356ac8e31492a948de5f25450a51d' # Denik2\n",
        "#API_SCHLÜSSEL = '694d6c2e280649b39f1ea1dcb6da05ed' # Denik3\n",
        "#API_SCHLÜSSEL = '022215c8920943eebbac8c759fb5cc10' #Denik 4\n",
        "#API_SCHLÜSSEL = '07e0bbd957f24f5aa3d1c5f8bb9a42ed'  #Julian\n",
        "\n",
        "# API-Endpunkt\n",
        "ENDPUNKT = 'https://newsapi.org/v2/everything'\n",
        "\n",
        "# Maximaler Zeitraum für die Anfrage (vom 02.09.2023 bis zum 29.09.2023)\n",
        "start_datum = '2023-09-02'\n",
        "end_datum = '2023-09-29'\n",
        "\n",
        "# Suchbegriffe\n",
        "suchbegriffe = [\n",
        "    'Apple',\n",
        "    'iPhone 15',\n",
        "    'iPhone 15 Plus',\n",
        "    'iPhone 15 Pro',\n",
        "    'iPhone 15 Pro Max',\n",
        "    'Apple Watch Series 9',\n",
        "    'Apple Watch Ultra 2',\n",
        "    'AirPods Pro (2. Generation)'\n",
        "]\n",
        "\n",
        "# Parameter für die Anfrage\n",
        "parameter = {\n",
        "    'language': 'de',  # Sprachparameter für deutsche Artikel\n",
        "    'sortBy': 'publishedAt',  # Sortiere Artikel nach Veröffentlichungsdatum\n",
        "    'apiKey': API_SCHLÜSSEL,\n",
        "    'from': start_datum,\n",
        "    'to': end_datum,\n",
        "    'page': 1\n",
        "}\n",
        "\n",
        "pageSize = 100\n",
        "maxArticlesPerKeyword = 2500\n",
        "\n",
        "# Zählvariable für die Gesamtzahl der Artikel\n",
        "gesamtzahl_artikel = 0\n",
        "\n",
        "# Erstellen Sie eine leere Liste, um die Artikel zu speichern\n",
        "alle_artikel = []\n",
        "\n",
        "# Durchlaufe die Suchbegriffe und führe separate Anfragen durch\n",
        "for suchbegriff in suchbegriffe:\n",
        "    parameter['q'] = suchbegriff  # Setze den aktuellen Suchbegriff in den Parameter\n",
        "\n",
        "    # Führe die GET-Anfrage durch\n",
        "    print(parameter)\n",
        "    antwort = requests.get(ENDPUNKT, params=parameter)\n",
        "\n",
        "    # Überprüfe, ob die Anfrage erfolgreich war\n",
        "    print(antwort.status_code)\n",
        "    daten = antwort.json()\n",
        "    print(\"Status:\", daten[\"status\"])\n",
        "    if antwort.status_code == 200:\n",
        "        anzahl_artikel = daten[\"totalResults\"]  # Anzahl der Artikel für den aktuellen Suchbegriff\n",
        "        gesamtzahl_artikel += anzahl_artikel\n",
        "        downloaded_articles = pageSize\n",
        "        artikel_liste = daten[\"articles\"]\n",
        "        alle_artikel.extend(artikel_liste)\n",
        "        result = True\n",
        "        while downloaded_articles < anzahl_artikel and downloaded_articles < maxArticlesPerKeyword and result:\n",
        "            parameter['page'] += 1\n",
        "            print(parameter)\n",
        "            antwort = requests.get(ENDPUNKT, params=parameter)\n",
        "            print(antwort.status_code)\n",
        "            daten = antwort.json()\n",
        "            print(\"Status:\", daten[\"status\"])\n",
        "            if antwort.status_code == 200:\n",
        "              artikel_liste = daten[\"articles\"]\n",
        "              alle_artikel.extend(artikel_liste)\n",
        "              downloaded_articles += pageSize\n",
        "            else:\n",
        "              result = False # Wenn Fehler, dann Schleife abbrechen\n",
        "        parameter['page'] = 1 # Parameter auf 1 zurücksetzen, um für den nächsten Suchbegriff wieder auf der ersten Seite zu beginnen\n",
        "\n",
        "# Erstellen Sie ein Pandas DataFrame aus den gesammelten Artikeln\n",
        "df = pd.DataFrame(alle_artikel)\n",
        "\n",
        "\n",
        "# Gesamtzahl der Artikel über alle Suchbegriffe hinweg ausgeben\n",
        "print(\"Gesamtzahl der Artikel über alle Suchbegriffe hinweg:\", gesamtzahl_artikel)"
      ],
      "metadata": {
        "id": "6at_FfGrMI3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "ozz8xiwXMelm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "BgwQKA1rMg7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop_duplicates(subset=['title', 'content', 'description'])\n",
        "df.info()"
      ],
      "metadata": {
        "id": "f7v_w7zzsJKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Frame in CSV speichern"
      ],
      "metadata": {
        "id": "CTBSyrP-Ycea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('articles.csv')"
      ],
      "metadata": {
        "id": "RoXJ0Vu5Ybux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Datenvalidierung und -aufbereitung**"
      ],
      "metadata": {
        "id": "mQVGRS9wyafj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vor der weiteren Analyse wird der Datensatz intensiv auf Anomalien\n",
        "untersucht. Im ersten Schritt wird sich ein Überblick über den Datensatz verschafft."
      ],
      "metadata": {
        "id": "i0o2iOvPyt1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_data = pd.read_csv('articles.csv')\n",
        "\n",
        "# Display information about the dataset (e.g., number of entries, data type of each column)\n",
        "print(full_data.info())\n",
        "print()  # Blank line for better readability\n",
        "\n",
        "# Display the number of missing values for each column\n",
        "print(full_data.isnull().sum())\n",
        "print()  # Blank line for better readability'\n",
        "\n",
        "# Display the first 5 rows of the dataset\n",
        "print(full_data.head())\n",
        "print()  # Blank line for better readability"
      ],
      "metadata": {
        "id": "57LRb_Y9zHdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Der erste Datensatz sieht nach einer Anaomalie aus und wird daher nachfolgend genauer betrachtet. Zur Visualisierung wird eine Tabelle erstellt, welche die Zeilen des ersten Datensatzes aus dem Dataframe wiedergibt"
      ],
      "metadata": {
        "id": "9-rO84aE0q7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the new DataFrame for visualization\n",
        "vis_df = pd.DataFrame({\n",
        "    '#': range(len(df.columns)),\n",
        "    'Column': df.columns,\n",
        "    'Non-Null': df.notnull().sum(),\n",
        "    'Example': df.iloc[0]\n",
        "})\n",
        "vis_df.set_index('#', inplace=True)\n",
        "\n",
        "# Save the DataFrame as a CSV file\n",
        "output_path = dateipfad.rsplit('/', 1)[0] + \"/table_visualization.csv\"\n",
        "vis_df.to_csv(output_path)\n",
        "\n",
        "vis_df"
      ],
      "metadata": {
        "id": "8fBrwOAk07d9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nach der Ansicht des ersten Artikels wird davon ausgegangen, dass der Datensatz auch Artikel beinhaltet die keine relevanten Informationen zu Apple enthält.\n",
        "Nachfolgend wird daher eine weitere Filterung der Daten durchgeführt. Die Suchbegriffe werden aus der Datenbeschaffung analog eingesetzt.\n",
        "\n",
        "Außerdem wird eine Bereinigung der Spalten source und publishedAT durchgeführt. Für die Spalte source soll nur den Text bestehen bleiben. Für publishedAt soll das Zeitformat nur das Datum beinhalten."
      ],
      "metadata": {
        "id": "Q9iTk1Jb1QZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of search terms\n",
        "search_terms = [\n",
        "    'Apple',\n",
        "    'iPhone 15',\n",
        "    'iPhone 15 Plus',\n",
        "    'iPhone 15 Pro',\n",
        "    'iPhone 15 Pro Max',\n",
        "    'Apple Watch Series 9',\n",
        "    'Apple Watch Ultra 2',\n",
        "    'Apple Watch',\n",
        "    'AirPods',\n",
        "    'AirPods Pro \\(2nd Generation\\)',  # Manuell escaped\n",
        "    'IOS',\n",
        "]\n",
        "\n",
        "# Escape regular expression special characters in search terms\n",
        "search_terms = [re.escape(term) for term in search_terms]\n",
        "\n",
        "# Create a regular expression from the search terms\n",
        "pattern = '|'.join(search_terms)\n",
        "\n",
        "# Filter DataFrame\n",
        "filtered_df = full_data[full_data[['title', 'content', 'description']].apply(lambda x: x.str.contains(pattern, case=False, na=False)).any(axis=1)].copy()\n",
        "\n",
        "# Display information about the dataset\n",
        "print(filtered_df.info())\n",
        "\n",
        "##Modifying the 'source' column for better overview\n",
        "# Function to extract the 'name' value from the string dictionary\n",
        "def extract_name(source_str):\n",
        "    try:\n",
        "        # Wandelt den String in ein Dictionary um\n",
        "        source_dict = ast.literal_eval(source_str)\n",
        "        return source_dict.get('name')\n",
        "    except:\n",
        "        return source_str\n",
        "\n",
        "# Apply the function to the 'source' column and edit 'publishedAt'\n",
        "filtered_df.loc[:, 'source'] = filtered_df['source'].apply(extract_name)\n",
        "filtered_df.loc[:, 'publishedAt'] = pd.to_datetime(filtered_df['publishedAt'])"
      ],
      "metadata": {
        "id": "Wi4pAAlZ1gTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check ob der Datensatz nur noch Artikel enthält, in denen bestimmte Schlüsselwörter vorhanden sind.\n",
        "\n"
      ],
      "metadata": {
        "id": "iAsGbGbq37tb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of search terms\n",
        "search_terms = [\n",
        "    'Apple',\n",
        "    'iPhone',\n",
        "    'Apple Watch',\n",
        "    'AirPods'\n",
        "]\n",
        "\n",
        "# Filter the DataFrame based on the search terms\n",
        "filtered_df_check = filtered_df[filtered_df.apply(lambda row: row.astype(str).str.contains('|'.join(search_terms), case=False).any(), axis=1)]\n",
        "\n",
        "# Find articles that do not match any of the search terms\n",
        "non_matching_df = filtered_df[~filtered_df.isin(filtered_df)].dropna()\n",
        "\n",
        "# Display the number of articles that do not match any of the search terms\n",
        "print(f\"Number of articles without any of the search terms: {len(non_matching_df)}\")\n",
        "\n",
        "# If there are articles that do not match any of the search terms, display some of them\n",
        "if len(non_matching_df) > 0:\n",
        "    print(\"\\nSome articles without the search terms:\")\n",
        "    display(non_matching_df.head())"
      ],
      "metadata": {
        "id": "LdBHOh3S4L0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jetzt wird untersucht, ob im Datensatz tatsächlich nur deutschsprachige Texte enthalten sind.\n",
        "Es werden die Spalten title, content und description mit einer Spracherkennung untersucht.\n",
        "Ziel ist es die Top Scores für englische Sprache zu ermitteln und ggf. englischsprachige Texte zu identifizieren."
      ],
      "metadata": {
        "id": "_l-rLip55L6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SpaCy-Modell\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# add Language Detector\n",
        "@Language.factory(\"language_detector\")\n",
        "def create_language_detector(nlp, name):\n",
        "    return LanguageDetector()\n",
        "\n",
        "nlp.add_pipe(\"language_detector\")\n",
        "\n",
        "# Function to get Englisch-Score\n",
        "def get_english_score(text):\n",
        "    doc = nlp(text)\n",
        "    if doc._.language['language'] == 'en':\n",
        "        return doc._.language['score']\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Get the top 10 scores for each column (Title, Description, Content)\n",
        "for column in ['title', 'description', 'content']:\n",
        "    # Apply the function to the entire column\n",
        "    filtered_df[f'{column}_en_score'] = filtered_df[column].astype(str).apply(get_english_score)\n",
        "\n",
        "    # Get the top 10 scores for this column and only display the selected columns\n",
        "    top10 = filtered_df.nlargest(10, f'{column}_en_score')[[column, 'source', f'{column}_en_score']]\n",
        "    print(f\"\\nTop 10 English Scores for {column}:\\n\", top10)"
      ],
      "metadata": {
        "id": "gHW7Oeto59wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trotz hoher Scores wurde kein englischsprachiger Artikel erkannt. Der teilweise hohe Score ist auf englischen Begriffe wie beispielsweise Apple oder  Watch zurückzuführen. Die Analyse hat jedoch aufgezeigt, dass im Datensatz Werbung enthalten ist. Insbesondere Quellen wie Amazon oder Dealdoktor scheinen Werbecontent zu enthalten. Ziel ist es nun diese Artikel ebenfalls herauszufiltern.\n",
        "\n",
        "Vorgehen:\n",
        "1. Offensichtliche Werbung mit bekannten Keywords filtern\n",
        "2. Quellen untersuchen und manuell überprüften\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SyWTOIYA6xPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of articles before filtering advertisements: {len(filtered_df)}\")\n",
        "\n",
        "# Keywords for Ads\n",
        "ad_keywords = [\"Kaufen Sie jetzt\", \"Werbung\", \"Sonderangebot\", \"Rabatt\", \"Bestellen Sie\", \"Anzeige\", \"Gewinnspiel\", \"Apple Podcasts\", \"Deal\", \"Schnäppchen\", \"schnaeppchen\", \"Tarif\"]\n",
        "\n",
        "# filter articles\n",
        "for keyword in ad_keywords:\n",
        "    filtered_df = filtered_df[~filtered_df['content'].str.contains(keyword, case=False, na=False)]\n",
        "\n",
        "print(f\"Number of remaining articles after filtering advertisements: {len(filtered_df)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "3Vo882Xs7NT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Es wurde Werbecontent mit den Schlüsselwörtern identifiziert und im Anschluss entfernt. Nachfolgend werden jetzt die Quellen untersucht."
      ],
      "metadata": {
        "id": "guh8PduF8Bff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the column containing the source is named 'source'\n",
        "unique_sources = filtered_df['source'].drop_duplicates().tolist()\n",
        "\n",
        "# Assuming the column containing the source is named 'source'\n",
        "source_counts = filtered_df['source'].value_counts()\n",
        "\n",
        "# Print the sources and their frequencies\n",
        "for source, count in source_counts.items():\n",
        "    print(f\"{source}: {count}\")"
      ],
      "metadata": {
        "id": "0ckF2qPN8Evf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insbesondere Amazon.de, Mein-deal.com, Dealdoktor.de und Kino-zeit.de sollten genauer untersucht werden."
      ],
      "metadata": {
        "id": "A0dbCPnW7M6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of sources to check\n",
        "sources_to_check = ['Amazon.de', 'Mein-deal.com', 'Dealdoktor.de', 'Kino-zeit.de']\n",
        "\n",
        "# Iterate through the list of sources\n",
        "for source in sources_to_check:\n",
        "    source_articles = filtered_df[filtered_df['source'] == source]\n",
        "\n",
        "    # Display the content and description for articles from this source\n",
        "    print(f\"--------- Articles from {source} ---------\")\n",
        "    for index, row in source_articles.iterrows():\n",
        "        print(f\"Title: {row['title']}\\n\")\n",
        "        print(f\"Description: {row['description']}\\n\")\n",
        "        print(f\"Content: {row['content']}\\n\")\n",
        "        print(\"----------------------------------------------------------\\n\")\n"
      ],
      "metadata": {
        "id": "efgX1En981F0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Artikel dieser Quellen können entfernt werden. Nun gilt es noch weitere Quellen zu validieren. Insbesondere unter den Quellen mit wenigen Häufigkeiten gibt es einige weitere unerwartete Quellen, wie beispielsweise Süddeutsche Zeitung. Deshalb werden nachfolgend alle Quellen die weniger als 10 mal im Datensatz vorhanden sind manuell überprüft. Der Fokus liegt auf den Spalten Title, Description und Content."
      ],
      "metadata": {
        "id": "qiUqX5QZ9E08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count how often each source occurs\n",
        "source_counts = filtered_df['source'].value_counts()\n",
        "\n",
        "# Filter out the sources that appear fewer than 10 times\n",
        "less_than_10_sources = source_counts[source_counts < 10].index.tolist()\n",
        "\n",
        "# Filter the DataFrame by these sources\n",
        "selected_articles = filtered_df[filtered_df['source'].isin(less_than_10_sources)]\n",
        "\n",
        "# Sort the DataFrame by the source\n",
        "selected_articles = selected_articles.sort_values(by='source')\n",
        "\n",
        "# Display the content and description of the selected articles\n",
        "for index, row in selected_articles.iterrows():\n",
        "    print(f\"Source: {row['source']}\")\n",
        "    print(f\"Title: {row['title']}\\n\")\n",
        "    print(f\"Description: {row['description']}\\n\")\n",
        "    print(f\"Content: {row['content']}\\n\")\n",
        "    print(\"----------------------------------------------------------\\n\")\n"
      ],
      "metadata": {
        "id": "CbWqr4w19EjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nach manueller Durchsicht der Quellen wurden weitere Quellen als nicht relevant klassifiziert. Nachfolgend werden diese aus dem Datensatz entfernt und der bereinigte Datensatz wird als neue Datei gespeichert."
      ],
      "metadata": {
        "id": "Gxrjp6hF9_Ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sources_to_remove = [\n",
        "    'Amazon.de',\n",
        "    'Mein-deal.com',\n",
        "    'Dealdoktor.de',\n",
        "    'Kino-zeit.de',\n",
        "    'Techstage.de',\n",
        "    'Süddeutsche Zeitung'\n",
        "    'Sparbote.de'\n",
        "    '4Players Portal'\n",
        "    'Die Zeit'\n",
        "]\n",
        "\n",
        "print(f\"Number of articles before removing certain sources: {len(filtered_df)}\")\n",
        "\n",
        "# Remove the rows that have one of the above sources as their source\n",
        "filtered_df = filtered_df[~filtered_df['source'].isin(sources_to_remove)]\n",
        "\n",
        "print(f\"Number of remaining articles after removing certain sources: {len(filtered_df)}\")\n",
        "\n",
        "# Save the new filtered DataFrame as a new CSV file\n",
        "filtered_df = filtered_df.drop(columns=['Unnamed: 0'])\n",
        "filtered_df.to_csv('cleaned_and_filtered_articles.csv')"
      ],
      "metadata": {
        "id": "_fAjb6wV9DYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zusammenfassung Datenvalidierung und -aufbereitung**\n",
        "\n",
        "Die API hat wahrscheinlich auch Datensätze erfasst, die bestimmte Wörter wie „Pro“, „Watch“ oder die Zahl 15 in den Texten enthalten. Der ursprünglich abgerufene Datensatz umfasste 1586 Artikel. Nach der Bereinigung von Datensätzen, die nicht auf Apple bezogen sind, sowie von Werbeartikeln, besteht der neue Datensatz nun aus 1175 Kurznachrichtenartikeln. Diese werden im Folgenden weiter bearbeitet.\n",
        "\n"
      ],
      "metadata": {
        "id": "VSCOjxsl-gh5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Datennachbearbeitung**"
      ],
      "metadata": {
        "id": "3ndNg7zsz5ZC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Frame aus CSV laden"
      ],
      "metadata": {
        "id": "QJtH0qBxZUbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_data = pd.read_csv('cleaned_and_filtered_articles.csv')\n",
        "full_data.head()"
      ],
      "metadata": {
        "id": "Q4h32S1KZWby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_data.info()"
      ],
      "metadata": {
        "id": "A-JPDed4Q1Kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_data['publishedAt'] = pd.to_datetime(full_data['publishedAt'])"
      ],
      "metadata": {
        "id": "Ps83FP3FYYWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_data.info()"
      ],
      "metadata": {
        "id": "Wwx1-YokZrQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_data = full_data.dropna() # leere Werte entfernen, da als Float interpretiert"
      ],
      "metadata": {
        "id": "fKq6Ly9-npLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenisierung"
      ],
      "metadata": {
        "id": "3X8p8RwKn_aT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def tokenize_text(texts):\n",
        "  tokenized_texts = []\n",
        "  for text in texts:\n",
        "    tokenized_texts.append(text.split())\n",
        "  # Sonderzeichen entfernen\n",
        "  cleaned_tokenized_texts = []\n",
        "  for text in tokenized_texts:\n",
        "    cleaned_words = []\n",
        "    for word in text:\n",
        "      word = re.sub(r'\\b\\w*?\\.\\.\\.\\w*?\\b', '', word) # entfernt Wörter mit \"...\"\n",
        "      word = re.sub('[^A-Za-z0-9üÜäÄöÖß]+', ' ', word) # Entfernt alle Sonderzeichen\n",
        "      word = re.sub(r'\\s+', ' ', word) # entfernt mehrfache Leerzeichen\n",
        "      if word != 'chars' and word.isalnum():\n",
        "        cleaned_words.append(word)\n",
        "    cleaned_tokenized_texts.append(cleaned_words)\n",
        "  return cleaned_tokenized_texts"
      ],
      "metadata": {
        "id": "wgVhS_6bLA7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_words(tokenized_texts):\n",
        "  word_count = Counter()\n",
        "  for text in tokenized_texts:\n",
        "    word_count.update(text)\n",
        "  return word_count"
      ],
      "metadata": {
        "id": "fF4v2mCHoQAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_data['title'] = tokenize_text(full_data['title'])\n",
        "full_data['description'] = tokenize_text(full_data['description'])\n",
        "full_data['content'] = tokenize_text(full_data['content'])"
      ],
      "metadata": {
        "id": "HlIuVxj5wva3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_data.head()"
      ],
      "metadata": {
        "id": "ZrPX5CSsxBzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_words(full_data['title']).most_common(10)"
      ],
      "metadata": {
        "id": "Do3tDfQyLx2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_words(full_data['description']).most_common(10)"
      ],
      "metadata": {
        "id": "jOuj_FtN02sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_words(full_data['content']).most_common(10)"
      ],
      "metadata": {
        "id": "9Qeys4ba03Vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(full_data['title'])"
      ],
      "metadata": {
        "id": "UTT_CvYwSBLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stopwörter entfernen"
      ],
      "metadata": {
        "id": "S7vkfxNIDY7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(nltk.corpus.stopwords.words('german'))\n",
        "def remove_stop(tokens):\n",
        "  cleaned_text = []\n",
        "  for text in tokens:\n",
        "    cleaned_text.append([t for t in text if t.lower() not in stopwords])\n",
        "  return cleaned_text"
      ],
      "metadata": {
        "id": "ILQUyoVqDaj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_data['title'] = remove_stop(full_data['title'])\n",
        "full_data['description'] = remove_stop(full_data['description'])\n",
        "full_data['content'] = remove_stop(full_data['content'])"
      ],
      "metadata": {
        "id": "6CFernUgx535"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_data.head()"
      ],
      "metadata": {
        "id": "5bMl7hjcyE4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatisierung"
      ],
      "metadata": {
        "id": "CbMOXIIw4td4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('de_core_news_sm')\n",
        "def lemma(texts):\n",
        "  lemmatized_texts = []\n",
        "  for text in texts:\n",
        "    lemmatized_words = ''\n",
        "    for word in text:\n",
        "      lemmatized_words = lemmatized_words + ' ' + nlp(word)[0].lemma_\n",
        "    lemmatized_texts.append(lemmatized_words)\n",
        "  return lemmatized_texts"
      ],
      "metadata": {
        "id": "Q1dZDkfq4uuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_data['title'] = lemma(full_data['title'])\n",
        "full_data['description'] = lemma(full_data['description'])\n",
        "full_data['content'] = lemma(full_data['content'])"
      ],
      "metadata": {
        "id": "tbwjIxLV59Ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_data.head()"
      ],
      "metadata": {
        "id": "parwAyFxGhwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_data.to_csv('full_data.csv')"
      ],
      "metadata": {
        "id": "UJn3hEh_Zp0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Datenexpolartion und -darstellung**"
      ],
      "metadata": {
        "id": "-fx-678tCJq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine text data from the 'title', 'description', and 'content' columns into one column.\n",
        "text_data = full_data['title'].astype(str) + \" \" + \\\n",
        "            full_data['description'].astype(str) + \" \" + \\\n",
        "            full_data['content'].astype(str)\n",
        "\n",
        "# Join all text rows into one large string.\n",
        "text = \" \".join(text_data)\n",
        "\n",
        "# Generate the WordCloud.\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "\n",
        "# Visualize the WordCloud.\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sFEsyCTYCJKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analyse"
      ],
      "metadata": {
        "id": "MA5Q8vGeL4hG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Daten importieren**"
      ],
      "metadata": {
        "id": "Y2wKZynVw9sb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definiere den Pfad zur CSV-Datei, die die Daten enthält.\n",
        "full_data = 'full_data.csv'\n",
        "\n",
        "# Lese die Daten aus der CSV-Datei in ein Pandas DataFrame namens 'df' ein.\n",
        "df = pd.read_csv(full_data)"
      ],
      "metadata": {
        "id": "rG6JZlfswyRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zeige die Form (Anzahl der Zeilen und Spalten) des DataFrames 'df'.\n",
        "print(df.shape)"
      ],
      "metadata": {
        "id": "47JLOw_HxHot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definiere eine Liste von Spaltennamen, die aus dem DataFrame 'df' ausgewählt werden sollen.\n",
        "selected_columns = ['Unnamed: 0', 'source', 'description', 'publishedAt']\n",
        "\n",
        "# Verwende die Liste von Spaltennamen, um diese Spalten aus dem DataFrame 'df' auszuwählen und ein neues DataFrame namens 'filtered_data' zu erstellen.\n",
        "filtered_data = df[selected_columns]\n",
        "\n",
        "# Zeige die ersten paar Zeilen des neuen DataFrames 'filtered_data', um das Ergebnis zu überprüfen.\n",
        "filtered_data.head()"
      ],
      "metadata": {
        "id": "_kdnXyFcxX7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <u>**TextBlobDE**</u>"
      ],
      "metadata": {
        "id": "h2S9MM8oxbi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Textvorschau anhand Zeile 685**"
      ],
      "metadata": {
        "id": "A-10kC6lyKOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob_de import TextBlobDE\n",
        "\n",
        "# Greife auf den Text in Zeile 685 des DataFrames zu und speichere ihn in 'example'.\n",
        "example = df.at[685, 'description']\n",
        "\n",
        "# Teile den Text in Wörter auf und wähle die ersten 10 Wörter aus.\n",
        "words = example.split()[:10]\n",
        "\n",
        "# Gib die ersten 10 Wörter aus.\n",
        "print(words)"
      ],
      "metadata": {
        "id": "zMmq9hpByZC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POS-Tagging anhand Zeile 685**"
      ],
      "metadata": {
        "id": "jJbsGYeXy3Bz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TextBlobDE-Objekt für den Satz erstellen\n",
        "blob = TextBlobDE(' '.join(words))\n",
        "\n",
        "# POS-Tagging für den Satz durchführen\n",
        "pos_tags = blob.tags  # Gibt eine Liste von Tupeln zurück (Wort, POS-Tag).\n",
        "\n",
        "# Ausgabe der POS-Tags\n",
        "print(\"POS-Tags für die ersten 10 Wörter:\", pos_tags)"
      ],
      "metadata": {
        "id": "E8hmIc9Wy8eD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <u>**TextBlobDE Sentiment-Analyse**<u>"
      ],
      "metadata": {
        "id": "gtckv_OCzX0Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentiment-Analyse an der Beispielzeile 685**"
      ],
      "metadata": {
        "id": "Q8PV027FzjzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TextBlobDE-Objekt für den Text in 'example' erstellen\n",
        "example_blob = TextBlobDE(example)\n",
        "\n",
        "# Sentiment-Analyse für den Text in 'example' durchführen\n",
        "sentiment = example_blob.sentiment\n",
        "\n",
        "# Ausgabe des Sentiments\n",
        "print(\"Sentiment-Analyse für Zeile 685:\")\n",
        "print(\"Polarity (Stimmung):\", sentiment.polarity)\n",
        "print(\"Subjectivity (Subjektivität):\", sentiment.subjectivity)"
      ],
      "metadata": {
        "id": "RbYJitcnzo6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Durchführung der Sentiment-Analyse für das gesamte DataFrame**"
      ],
      "metadata": {
        "id": "HsCJJGs10k1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Listen zum Speichern der Ergebnisse erstellen\n",
        "sentiments = []\n",
        "polarities = []\n",
        "subjectivities = []\n",
        "\n",
        "# Durchlaufen jeder Zeile der 'description'-Spalte\n",
        "for index, row in df.iterrows():\n",
        "    # Text aus der 'description'-Spalte extrahieren und sicherstellen, dass es ein String ist\n",
        "    text = str(row['description'])\n",
        "\n",
        "    # TextBlobDE-Objekt für den Text erstellen\n",
        "    blob = TextBlobDE(text)\n",
        "\n",
        "    # Sentiment-Analyse für den Text durchführen\n",
        "    sentiment = blob.sentiment\n",
        "\n",
        "    # Ergebnisse speichern\n",
        "    sentiments.append(sentiment)\n",
        "    polarities.append(sentiment.polarity)\n",
        "    subjectivities.append(sentiment.subjectivity)\n",
        "\n",
        "# Ergebnisse in DataFrame speichern\n",
        "df['sentiment'] = sentiments\n",
        "df['polarity'] = polarities\n",
        "df['subjectivity'] = subjectivities\n",
        "\n",
        "# Anzeigen der ersten paar Zeilen des DataFrames mit Ergebnissen\n",
        "print(df[['description', 'sentiment', 'polarity', 'subjectivity']].head())"
      ],
      "metadata": {
        "id": "CTvcoJ-v0pLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Anzeigen der einzelnen Polarity-Werte**"
      ],
      "metadata": {
        "id": "C1baEuxm0uVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zählen der einzigartigen Polarity-Werte und speichern in einer Serie\n",
        "polarity_counts = df['polarity'].value_counts()\n",
        "\n",
        "# Konvertieren der Serie in ein DataFrame und hinzufügen von Spaltenüberschriften\n",
        "polarity_counts_df = pd.DataFrame({'Polarity': polarity_counts.index, 'Anzahl': polarity_counts.values})\n",
        "\n",
        "# Sortieren des DataFrame nach der Polarity\n",
        "polarity_counts_df = polarity_counts_df.sort_values(by='Polarity')\n",
        "\n",
        "# Ausgabe des DataFrames mit der Anzahl der Polarity-Werte\n",
        "print(polarity_counts_df)"
      ],
      "metadata": {
        "id": "ZrNGMr-V0yE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ausgabe der Häufigkeit von Sentiments basierend auf der Polarität**"
      ],
      "metadata": {
        "id": "rc14wrql02r-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Häufigkeit der Sentiments ausgeben\n",
        "positive_sentiments = len(df[df['polarity'] > 0])\n",
        "neutral_sentiments = len(df[df['polarity'] == 0])\n",
        "negative_sentiments = len(df[df['polarity'] < 0])\n",
        "\n",
        "print(\"Anzahl der positiven Sentiments:\", positive_sentiments)\n",
        "print(\"Anzahl der neutralen Sentiments:\", neutral_sentiments)\n",
        "print(\"Anzahl der negativen Sentiments:\", negative_sentiments)"
      ],
      "metadata": {
        "id": "Uvg20Fzf06I8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ausgabe drei positive Nachrichten**"
      ],
      "metadata": {
        "id": "CeEtnui208uN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DataFrame nach der Polarität (Sentiment) sortieren, absteigend (positiv zuerst)\n",
        "sorted_df = df.sort_values(by='polarity', ascending=False)\n",
        "\n",
        "# Die drei positivsten Nachrichten auswählen\n",
        "top_3_positive_news = sorted_df.head(3)\n",
        "\n",
        "# Die ausgewählten Nachrichten anzeigen\n",
        "print(\"Drei positive Nachrichten:\")\n",
        "for index, row in top_3_positive_news.iterrows():\n",
        "    print(\"Nachricht:\", row['description'])\n",
        "    print(\"Polarity (Sentiment):\", row['polarity'])\n",
        "    print()"
      ],
      "metadata": {
        "id": "TJgsdt791Dqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ausgabe drei negative Nachrichten**"
      ],
      "metadata": {
        "id": "X8jV9URB1MLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DataFrame nach der Polarität (Sentiment) sortieren, aufsteigend (negativ zuerst)\n",
        "sorted_df = df.sort_values(by='polarity', ascending=True)\n",
        "\n",
        "# Die drei negativsten Nachrichten auswählen\n",
        "top_3_negative_news = sorted_df.head(3)\n",
        "\n",
        "# Die ausgewählten Nachrichten anzeigen\n",
        "print(\"Drei negative Nachrichten:\")\n",
        "for index, row in top_3_negative_news.iterrows():\n",
        "    print(\"Nachricht:\", row['description'])\n",
        "    print(\"Polarity (Sentiment):\", row['polarity'])\n",
        "    print()"
      ],
      "metadata": {
        "id": "G0wdmJhI1HPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Durchschnittliche Subjektivität aller Nachrichten**"
      ],
      "metadata": {
        "id": "e1CPqFS91RwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Durchschnitt der Subjektivitäten aus der 'subjectivity'-Spalte des DataFrames berechnen\n",
        "average_subjectivity = df['subjectivity'].mean()\n",
        "\n",
        "# Durchschnitt auf zwei Nachkommastellen runden\n",
        "average_subjectivity = round(average_subjectivity, 2)\n",
        "\n",
        "# Durchschnittliche Subjektivität aller Nachrichten ausgeben\n",
        "print(\"Durchschnittliche Subjektivität aller Nachrichten:\", average_subjectivity)"
      ],
      "metadata": {
        "id": "DxP80uC-1VQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Durchschnittliche Subjektivität nach Source (absteigend)**"
      ],
      "metadata": {
        "id": "x4qCITjy1ZQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "# Gruppieren der Daten nach der Quellenspalte\n",
        "grouped_data = df.groupby('source')\n",
        "\n",
        "# Durchschnitt der Subjektivitäten und die Anzahl der Nachrichten pro Quelle berechnen\n",
        "average_subjectivity_per_source = grouped_data['subjectivity'].mean()\n",
        "count_per_source = grouped_data.size()\n",
        "\n",
        "# Durchschnittliche Subjektivität auf 2 Nachkommastellen runden\n",
        "average_subjectivity_per_source = average_subjectivity_per_source.round(2)\n",
        "\n",
        "# Erstellung DataFrame, um die Ergebnisse zusammenzufassen\n",
        "result_df = pd.DataFrame({'Quelle': average_subjectivity_per_source.index, 'Durchschnittliche Subjektivität': average_subjectivity_per_source.values, 'Anzahl Nachrichten': count_per_source.values})\n",
        "\n",
        "# Ergebnisse nach der durchschnittlichen Subjektivität in absteigender Reihenfolge sortieren\n",
        "result_df = result_df.sort_values(by='Durchschnittliche Subjektivität', ascending=False)\n",
        "\n",
        "# Top 5 Quellen auswählen\n",
        "top_5_sources = result_df.head(5)\n",
        "\n",
        "# Tabulate verwenden, um die Top 5 Quellen in einer Tabelle zu formatieren\n",
        "table = tabulate(top_5_sources, headers='keys', tablefmt='pretty', showindex=False)\n",
        "\n",
        "# Ausgabe der formatierten Tabelle\n",
        "print(table)"
      ],
      "metadata": {
        "id": "KTa3CWXb1cWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Durchschnittliches Sentiment (gesamt)**"
      ],
      "metadata": {
        "id": "mr5NFxuB1g_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Durchschnittliche Polarität (Sentiment) berechnen\n",
        "average_sentiment = df['polarity'].mean()\n",
        "\n",
        "# Durchschnitt auf zwei Nachkommastellen runden\n",
        "average_sentiment = round(average_sentiment, 2)\n",
        "\n",
        "# Ausgabe des durchschnittlichen Sentiments\n",
        "print(\"Durchschnittliches Sentiment der Nachrichten:\", average_sentiment)"
      ],
      "metadata": {
        "id": "tOShGr_H1kRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Durchschnittliches Sentiment (vor dem 12.09.2023 17:00)**"
      ],
      "metadata": {
        "id": "c-76KwYG1n76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytz import timezone\n",
        "\n",
        "# 'publishedAt'-Spalte in einen datetime-Datentyp mit UTC-Zeitzone konvertieren\n",
        "df['publishedAt'] = pd.to_datetime(df['publishedAt'], utc=True)\n",
        "\n",
        "# Schwellenwert für das Datum und die Uhrzeit mit UTC-Zeitzone setzen\n",
        "threshold_datetime = pd.Timestamp('2023-09-12 17:00:00', tz=timezone('UTC'))\n",
        "\n",
        "# Nachrichten vor dem Schwellenwert filtern\n",
        "filtered_df = df[df['publishedAt'] < threshold_datetime]\n",
        "\n",
        "# Das durchschnittliche Sentiment für die ausgewählten Nachrichten berechnen\n",
        "average_sentiment = filtered_df['polarity'].mean()\n",
        "\n",
        "# Durchschnitt auf zwei Nachkommastellen runden\n",
        "average_sentiment = round(average_sentiment, 2)\n",
        "\n",
        "# Ausgabe des durchschnittlichen Sentiments\n",
        "print(\"Durchschnittliches Sentiment der Nachrichten vor dem 12.09.2023 um 17:00 Uhr:\", average_sentiment)"
      ],
      "metadata": {
        "id": "Gs1Ke4ua1vgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Durchschnittliches Sentiment (nach dem 12.09.2023 17:00)**"
      ],
      "metadata": {
        "id": "QR2KUdEC1xUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytz import timezone\n",
        "\n",
        "# 'publishedAt'-Spalte in einen datetime-Datentyp mit UTC-Zeitzone konvertieren\n",
        "df['publishedAt'] = pd.to_datetime(df['publishedAt'], utc=True)\n",
        "\n",
        "# Schwellenwert für das Datum und die Uhrzeit auf 12.09.2023 um 17:00:00 mit UTC-Zeitzone setzen\n",
        "threshold_datetime = pd.Timestamp('2023-09-12 17:00:00', tz=timezone('UTC'))\n",
        "\n",
        "# Nachrichten nach dem Schwellenwert filtern\n",
        "filtered_df = df[df['publishedAt'] > threshold_datetime]\n",
        "\n",
        "# Das durchschnittliche Sentiment für die ausgewählten Nachrichten berechnen\n",
        "average_sentiment = filtered_df['polarity'].mean()\n",
        "\n",
        "# Durchschnitt auf zwei Nachkommastellen runden\n",
        "average_sentiment = round(average_sentiment, 2)\n",
        "\n",
        "# Ausgabe des durchschnittlichen Sentiments\n",
        "print(\"Durchschnittliches Sentiment der Nachrichten nach dem 12.09.2023 um 17:00 Uhr:\", average_sentiment)"
      ],
      "metadata": {
        "id": "UVm5zB5g10U8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualisierung des durchschnittlichen Sentiments pro Tag**"
      ],
      "metadata": {
        "id": "ZmPoQTbx2GlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 'publishedAt'-Spalte in einen datetime-Datentyp mit UTC-Zeitzone konvertieren\n",
        "df['publishedAt'] = pd.to_datetime(df['publishedAt'], utc=True)\n",
        "\n",
        "# Daten nach Tag gruppieren und das durchschnittliche Sentiment pro Tag berechnen\n",
        "average_sentiment_per_day = df.groupby(df['publishedAt'].dt.date)['polarity'].mean().reset_index()\n",
        "\n",
        "# Diagramm erstellen, um das durchschnittliche Sentiment pro Tag anzuzeigen\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(data=average_sentiment_per_day, x='publishedAt', y='polarity', marker='o')\n",
        "plt.title('Durchschnittliches Sentiment pro Tag')\n",
        "plt.xlabel('Datum')\n",
        "plt.ylabel('Durchschnittliche Polarität (Sentiment)')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Anzeigen der Grafik\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "udY-IxiS2I0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ausgabe Tief- und Hochpunkt des Durchschnitts der Polarität**"
      ],
      "metadata": {
        "id": "dqtXocIg42rO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Datum mit dem niedrigsten Durchschnitt der Polarität finden\n",
        "date_lowest_sentiment = average_sentiment_per_day.loc[average_sentiment_per_day['polarity'].idxmin(), 'publishedAt']\n",
        "lowest_sentiment = round(average_sentiment_per_day['polarity'].min(), 2)\n",
        "\n",
        "# Datum mit dem höchsten Durchschnitt der Polarität finden\n",
        "date_highest_sentiment = average_sentiment_per_day.loc[average_sentiment_per_day['polarity'].idxmax(), 'publishedAt']\n",
        "highest_sentiment = round(average_sentiment_per_day['polarity'].max(), 2)\n",
        "\n",
        "# Ausgabe des Tiefpunkts und Hochpunkts\n",
        "print(\"Tiefpunkt (niedrigster Durchschnitt der Polarität):\")\n",
        "print(\"Datum:\", date_lowest_sentiment)\n",
        "print(\"Durchschnittliche Polarität:\", lowest_sentiment)\n",
        "\n",
        "print(\"\\nHochpunkt (höchster Durchschnitt der Polarität):\")\n",
        "print(\"Datum:\", date_highest_sentiment)\n",
        "print(\"Durchschnittliche Polarität:\", highest_sentiment)\n",
        "\n",
        "# Anzeigen der Grafik\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pm5F0J_646C0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analyse der durchschnittlichen Polarität nach Quelle**"
      ],
      "metadata": {
        "id": "qgb2CL6u49DK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gruppiere den DataFrame nach der Source und berechne den Durchschnitt der Polarität für jede Source\n",
        "source_sentiment_avg = df.groupby('source')['polarity'].agg(['mean', 'count']).reset_index()\n",
        "\n",
        "# Runde den Durchschnitt der Polarität auf zwei Nachkommastellen\n",
        "source_sentiment_avg['mean'] = source_sentiment_avg['mean'].round(2)\n",
        "\n",
        "# Sortiere die Quellen absteigend nach der durchschnittlichen Polarität\n",
        "sorted_sources = source_sentiment_avg.sort_values(by='mean', ascending=False)\n",
        "\n",
        "# Entferne den Index (Zahlen am Anfang)\n",
        "sorted_sources.index = sorted_sources['source']\n",
        "sorted_sources = sorted_sources[['mean', 'count']]\n",
        "\n",
        "# Ausgabe der besten Quellen (absteigend nach durchschnittlicher Polarität) mit Anzahl der Artikel\n",
        "print(\"Die besten Quellen (absteigend nach durchschnittlicher Polarität) und Anzahl der Artikel:\")\n",
        "print(sorted_sources.head())\n",
        "\n",
        "# Ausgabe der schlechtesten Quellen (absteigend nach durchschnittlicher Polarität) mit Anzahl der Artikel\n",
        "print(\"\\nDie schlechtesten Quellen (absteigend nach durchschnittlicher Polarität) und Anzahl der Artikel:\")\n",
        "print(sorted_sources.tail())"
      ],
      "metadata": {
        "id": "eDQ7rtQY5A2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <u>**German Sentiment Classification with Bert**<u>"
      ],
      "metadata": {
        "id": "uy9NgOdx5Eh7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentiment-Analyse an der Beispielzeile 685**"
      ],
      "metadata": {
        "id": "mPQGt66_5S44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisiere das Sentiment-Modell\n",
        "model = SentimentModel()\n",
        "\n",
        "# Greife auf den Text in Zeile 685 des DataFrames zu und speichere ihn in 'example'.\n",
        "example = df.at[685, 'description']\n",
        "\n",
        "# Führe die Sentiment-Analyse auf dem Beispieltext durch\n",
        "result = model.predict_sentiment([example])\n",
        "print(result)"
      ],
      "metadata": {
        "id": "IV8hgGIt5WUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentiment-Analyse an der Beispielzeile 685 inkl. Wahrscheinlichkeiten**"
      ],
      "metadata": {
        "id": "wYGRsY_w5hpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Führe die Sentiment-Analyse auf dem Beispieltext durch\n",
        "classes, probabilities = model.predict_sentiment([example], output_probabilities=True)\n",
        "print(classes, probabilities)"
      ],
      "metadata": {
        "id": "LapbYiha5kp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Durchführung der Sentiment-Analyse für das gesamte DataFrame**"
      ],
      "metadata": {
        "id": "54m9s4Tw5nh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Erstelle leere Listen, um die Ergebnisse zu speichern\n",
        "sentiments = []\n",
        "probabilities_list = []\n",
        "\n",
        "# Iteriere durch alle Zeilen in der Spalte 'description' des DataFrames\n",
        "for index, row in df.iterrows():\n",
        "    text = row['description']\n",
        "\n",
        "    # Überprüfe, ob der Wert vom Typ String ist\n",
        "    if isinstance(text, str):\n",
        "        # Führe die Sentiment-Analyse auf dem aktuellen Text durch\n",
        "        classes, probabilities = model.predict_sentiment([text], output_probabilities=True)\n",
        "\n",
        "        # Füge die Ergebnisse zur jeweiligen Liste hinzu\n",
        "        sentiments.append(classes[0])\n",
        "        probabilities_list.append(probabilities[0])\n",
        "    else:\n",
        "        # Falls der Wert kein Text ist, füge Platzhalterwerte hinzu\n",
        "        sentiments.append(None)\n",
        "        probabilities_list.append(None)\n",
        "\n",
        "# Füge die Ergebnisse als neue Spalten zum DataFrame hinzu\n",
        "df['sentiment'] = sentiments\n",
        "df['sentiment_probabilities'] = probabilities_list"
      ],
      "metadata": {
        "id": "dfEvZdcn5qkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ausgabe der Ergebnisse der Sentiment-Analyse**"
      ],
      "metadata": {
        "id": "6O6Skdq55uf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iteriere durch den DataFrame und gib die Ergebnisse aus\n",
        "for index, row in df.iterrows():\n",
        "    description = row['description']\n",
        "    sentiment = row['sentiment']\n",
        "    probabilities = row['sentiment_probabilities']\n",
        "\n",
        "    print(f\"Text: {description}\")\n",
        "    print(f\"Sentiment: {sentiment}\")\n",
        "    print(f\"Sentiment Wahrscheinlichkeiten: {probabilities}\")\n",
        "    print(\"-\" * 50)  # Trennlinie zwischen den Texten"
      ],
      "metadata": {
        "id": "zHE7KLta50yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ausgabe der Sentiment-Häufigkeit und Prozentsätze**"
      ],
      "metadata": {
        "id": "SQpEHePH7hdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtere die Zeilen aus, bei denen das Sentiment nicht fehlt\n",
        "filtered_df = df.dropna(subset=['sentiment'])\n",
        "\n",
        "# Zähle die Anzahl der Vorkommnisse jedes Sentiments\n",
        "sentiment_counts = filtered_df['sentiment'].value_counts()\n",
        "\n",
        "# Berechne Prozentsätze für jedes Sentiment\n",
        "sentiment_percentages = (sentiment_counts / sentiment_counts.sum()) * 100\n",
        "\n",
        "# Gib die Anzahl der Vorkommnisse und Prozentsätze aus\n",
        "print(\"Anzahl der Vorkommnisse jedes Sentiments:\")\n",
        "print(sentiment_counts)\n",
        "print(\"\\nProzentsätze jedes Sentiments:\")\n",
        "print(sentiment_percentages)"
      ],
      "metadata": {
        "id": "cfFT-fhy7lt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ausgabe der drei positivsten Artikel inkl. Wahrscheinlichkeiten**"
      ],
      "metadata": {
        "id": "jiizTG1G7rFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sortiere den DataFrame nach Sentiment in absteigender Reihenfolge, um die positivsten Artikel zuerst zu erhalten\n",
        "df_sorted = df.sort_values(by='sentiment', ascending=False)\n",
        "\n",
        "# Gib die drei positivsten Artikel in ganzen Sätzen aus, inklusive der Wahrscheinlichkeit\n",
        "for i, row in df_sorted.head(3).iterrows():\n",
        "    description = row['description']\n",
        "    sentiment = row['sentiment']\n",
        "    probabilities = row['sentiment_probabilities']\n",
        "\n",
        "    print(f\"Artikel {i + 1}:\")\n",
        "    print(f\"Text: {description}\")\n",
        "    print(f\"Sentiment: {sentiment}\")\n",
        "    print(f\"Sentiment Wahrscheinlichkeiten: {probabilities}\")\n",
        "    print(\"-\" * 50)  # Trennlinie zwischen den Artikeln"
      ],
      "metadata": {
        "id": "tzQFJ5Xu7uul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ausgabe der drei negativsten Artikel inkl. Wahrscheinlichkeiten**"
      ],
      "metadata": {
        "id": "ujr7kLLw7xwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sortiere den DataFrame nach Sentiment in aufsteigender Reihenfolge, um die negativsten Artikel zuerst zu erhalten\n",
        "df_sorted = df.sort_values(by='sentiment', ascending=True)\n",
        "\n",
        "# Gib die drei negativsten Artikel in ganzen Sätzen aus, inklusive der Wahrscheinlichkeit\n",
        "for i, row in df_sorted.head(3).iterrows():\n",
        "    description = row['description']\n",
        "    sentiment = row['sentiment']\n",
        "    probabilities = row['sentiment_probabilities']\n",
        "\n",
        "    print(f\"Artikel {i + 1}:\")\n",
        "    print(f\"Text: {description}\")\n",
        "    print(f\"Sentiment: {sentiment}\")\n",
        "    print(f\"Sentiment Wahrscheinlichkeiten: {probabilities}\")\n",
        "    print(\"-\" * 50)  # Trennlinie zwischen den Artikeln"
      ],
      "metadata": {
        "id": "VRU49xU7701J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Verteilung der Sentiments (Gesamt)**"
      ],
      "metadata": {
        "id": "awcW50ao8Dpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gruppiere die Daten nach dem Sentiment und zähle die Anzahl in jeder Gruppe\n",
        "sentiment_counts = df['sentiment'].value_counts()\n",
        "\n",
        "# Übersetzung von ursprünglichen Beschreibungen zu neuen Beschreibungen\n",
        "sentiment_mapping = {\n",
        "    'negative': 'Negativ',\n",
        "    'positive': 'Positiv',\n",
        "    'neutral': 'Neutral'\n",
        "}\n",
        "\n",
        "# Mapping anwenden, um die Sentiments umzubenennen\n",
        "sentiment_counts.index = sentiment_counts.index.map(sentiment_mapping)\n",
        "\n",
        "# Benutzerdefinierte Farben für die Slices\n",
        "colors = ['#ff9999', '#66b3ff', '#99ff99']\n",
        "\n",
        "# Erstelle ein Tortendiagramm\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=140, colors=colors)\n",
        "plt.title('Verteilung der Sentiments')\n",
        "\n",
        "# Hinzufügen einer Legende außerhalb des Diagramms\n",
        "plt.legend(sentiment_counts.index, title='Sentiments', loc='upper right', bbox_to_anchor=(1.25, 1))\n",
        "\n",
        "# Zeige das Diagramm an\n",
        "plt.axis('equal')  # Stellt sicher, dass das Diagramm kreisförmig ist\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q5PeDd1b8Gkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Verteilung der Sentiments vor dem 12.09.2023 17 Uhr**"
      ],
      "metadata": {
        "id": "L3TWFKHS8Kfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Daten vor dem 12.09.2023 um 17 Uhr filtern\n",
        "filtered_df = df[df['publishedAt'] < '2023-09-12 17:00:00']\n",
        "\n",
        "# Gruppiere die gefilterten Daten nach dem Sentiment und zähle die Anzahl in jeder Gruppe\n",
        "sentiment_counts = filtered_df['sentiment'].value_counts()\n",
        "\n",
        "# Übersetzung von ursprünglichen Beschreibungen zu neuen Beschreibungen\n",
        "sentiment_mapping = {\n",
        "    'negative': 'Negativ',\n",
        "    'positive': 'Positiv',\n",
        "    'neutral': 'Neutral'\n",
        "}\n",
        "\n",
        "# Mapping anwenden, um die Sentiments umzubenennen\n",
        "sentiment_counts.index = sentiment_counts.index.map(sentiment_mapping)\n",
        "\n",
        "# Benutzerdefinierte Farben für die Slices\n",
        "colors = ['#ff9999', '#66b3ff', '#99ff99']\n",
        "\n",
        "# Erstelle ein Tortendiagramm\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=140, colors=colors)\n",
        "plt.title('Verteilung der Sentiments (vor 12.09.2023, 17:00 Uhr)')\n",
        "\n",
        "# Hinzufügen einer Legende außerhalb des Diagramms\n",
        "plt.legend(sentiment_counts.index, title='Sentiments', loc='upper right', bbox_to_anchor=(1.25, 1))\n",
        "\n",
        "# Zeige das Diagramm an\n",
        "plt.axis('equal')  # Stellt sicher, dass das Diagramm kreisförmig ist\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ys1UOefV8Nz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Verteilung der Sentiments nach dem 12.09.2023 17 Uhr**"
      ],
      "metadata": {
        "id": "-Trojiqd8Q4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Daten nach dem 12.09.2023 um 17 Uhr filtern\n",
        "filtered_df = df[df['publishedAt'] > '2023-09-12 17:00:00']\n",
        "\n",
        "# Gruppiere die gefilterten Daten nach dem Sentiment und zähle die Anzahl in jeder Gruppe\n",
        "sentiment_counts = filtered_df['sentiment'].value_counts()\n",
        "\n",
        "# Übersetzung von ursprünglichen Beschreibungen zu neuen Beschreibungen\n",
        "sentiment_mapping = {\n",
        "    'negative': 'Negativ',\n",
        "    'positive': 'Positiv',\n",
        "    'neutral': 'Neutral'\n",
        "}\n",
        "\n",
        "# Mapping anwenden, um die Sentiments umzubenennen\n",
        "sentiment_counts.index = sentiment_counts.index.map(sentiment_mapping)\n",
        "\n",
        "# Benutzerdefinierte Farben für die Slices\n",
        "colors = ['#ff9999', '#66b3ff', '#99ff99']\n",
        "\n",
        "# Manuell die Prozentsätze berechnen und auf 100% runden\n",
        "total = sentiment_counts.sum()\n",
        "percentages = [(count / total) * 100 for count in sentiment_counts]\n",
        "rounded_percentages = [round(percentage, 1) for percentage in percentages]\n",
        "\n",
        "# Erstelle ein Tortendiagramm\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.pie(rounded_percentages, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=140, colors=colors)\n",
        "plt.title('Verteilung der Sentiments (nach 12.09.2023, 17:00 Uhr)')\n",
        "\n",
        "# Hinzufügen einer Legende außerhalb des Diagramms\n",
        "plt.legend(sentiment_counts.index, title='Sentiments', loc='upper right', bbox_to_anchor=(1.25, 1))\n",
        "\n",
        "# Zeige das Diagramm an\n",
        "plt.axis('equal')  # Stellt sicher, dass das Diagramm kreisförmig ist\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L9oXrGmy8Ubb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Themenmodellierung"
      ],
      "metadata": {
        "id": "65YnVhY9L9-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF berechnen:\n"
      ],
      "metadata": {
        "id": "Mgy-iA0borkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "# Get german stopwords (append, if necessary)\n",
        "stopwords = stopwords.words('german')\n",
        "#stopwords.extend(['sowie','36','de','indeed','workwise']) # Beispiel, ggf. erweitern"
      ],
      "metadata": {
        "id": "sH-_D8k-qxz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We need vectorized data\n",
        "# Let’s calculate the TF-IDF matrix both for the descriptions and for the paragraphs.\n",
        "tfidf_text_vectorizer = TfidfVectorizer(stop_words=stopwords, min_df=5, max_df=0.7)\n",
        "tfidf_text_vectors = tfidf_text_vectorizer.fit_transform(full_data['description'])\n",
        "tfidf_text_vectors.shape"
      ],
      "metadata": {
        "id": "w6ZFg5uiMBW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matrizen berechnen"
      ],
      "metadata": {
        "id": "HOV3p5KBp148"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nmf_text_model = NMF(n_components=10, random_state=42) # n_components: number of topics\n",
        "W_text_matrix = nmf_text_model.fit_transform(tfidf_text_vectors)\n",
        "H_text_matrix = nmf_text_model.components_\n",
        "W_text_matrix"
      ],
      "metadata": {
        "id": "AwENMYHNp3Od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Themen ermitteln"
      ],
      "metadata": {
        "id": "ZklcTjkTpr2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_topics(model, features, no_top_words=5):\n",
        "    for topic, words in enumerate(model.components_):\n",
        "        total = words.sum()\n",
        "        largest = words.argsort()[::-1] # invert sort order\n",
        "        print(\"\\nTopic %02d\" % topic)\n",
        "        for i in range(0, no_top_words):\n",
        "            print(\"  %s (%2.2f)\" % (features[largest[i]], abs(words[largest[i]]*100.0/total)))\n",
        "\n",
        "display_topics(nmf_text_model, tfidf_text_vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "Rofh_xEqptGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LDA"
      ],
      "metadata": {
        "id": "xE1Puquv3J28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "# Define the number of topics or components\n",
        "num_components=8\n",
        "\n",
        "# Create LDA object\n",
        "model=LatentDirichletAllocation(n_components=num_components)\n",
        "\n",
        "# Fit and Transform SVD model on data\n",
        "lda_matrix = model.fit_transform(tfidf_text_vectors)\n",
        "\n",
        "# Get Components\n",
        "lda_components=model.components_"
      ],
      "metadata": {
        "id": "G6VMovmD3KwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the topics with their terms\n",
        "terms = tfidf_text_vectorizer.get_feature_names_out()\n",
        "\n",
        "for index, component in enumerate(lda_components):\n",
        "    zipped = zip(terms, component)\n",
        "    top_terms_key=sorted(zipped, key = lambda t: t[1], reverse=True)[:7]\n",
        "    top_terms_list=list(dict(top_terms_key).keys())\n",
        "    print(\"Topic \"+str(index)+\": \",top_terms_list)"
      ],
      "metadata": {
        "id": "9dlUzXnX5TOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lda_matrix)"
      ],
      "metadata": {
        "id": "GKjykxw8BPur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count_para_vectorizer = CountVectorizer(stop_words=stopwords, min_df=5, max_df=0.7)\n",
        "count_para_vectors = count_para_vectorizer.fit_transform(full_data['description'])\n",
        "\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "lda_para_model = LatentDirichletAllocation(n_components = 10, random_state=42)\n",
        "W_lda_para_matrix = lda_para_model.fit_transform(count_para_vectors)\n",
        "H_lda_para_matrix = lda_para_model.components_\n",
        "\n",
        "display_topics(lda_para_model, count_para_vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "uRn2COCMDgFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "pyLDAvis.enable_notebook()\n",
        "lda_display = pyLDAvis.gensim.prepare(model, lda_matrix, lda_components)"
      ],
      "metadata": {
        "id": "lJpyq2A76WVo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}